{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ProcessGAN - AI-Enabled Business Process Improvement\n",
    "## 1 Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from random import sample\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import pm4py\n",
    "from pm4py.objects.conversion.log import converter as log_converter\n",
    "from pm4py.objects.log.importer.xes import importer as xes_importer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Input and Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"placeholder\"\n",
    "output_path = \"placeholder\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Import and convert XES data\n",
    "log = xes_importer.apply(input_path)\n",
    "log = log_converter.apply(log, variant = log_converter.Variants.TO_DATA_FRAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Import CSV data\n",
    "log = pd.read_csv(input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select and rename columns from dataset (to be adapted to each dataset)\n",
    "org_df = log[[\"concept:name\",\"lifecycle:transition\", \"time:timestamp\",\"case:concept:name\"]]\n",
    "org_df.columns = [\"activity\", \"status\", \"timestamp\", \"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert columns to specified types\n",
    "def convert_datatypes(df, datatypes, timestamp_col, activity_col, id_col):\n",
    "    df.loc[:,timestamp_col] = pd.to_datetime(df.loc[:,timestamp_col], utc = True)\n",
    "    \n",
    "    df = df.astype(datatypes)\n",
    "       \n",
    "    time_index = df.columns.get_loc(timestamp_col)\n",
    "    act_index = df.columns.get_loc(activity_col)\n",
    "    id_index = df.columns.get_loc(id_col)\n",
    "    \n",
    "    # Calculate activity duration\n",
    "    time_differences = []\n",
    "    for i in range(1, len(df), 2):\n",
    "        start = df.iloc[i-1, time_index]\n",
    "        complete = df.iloc[i, time_index]   \n",
    "        duration = (complete - start).total_seconds()\n",
    "        time_differences.extend([duration, duration])\n",
    "    df[\"duration\"] = time_differences\n",
    "    dur_index = df.columns.get_loc(\"duration\")\n",
    "      \n",
    "    # Calculate time since start of the trace\n",
    "    start_time_per_id = []\n",
    "    for i in df[id_col].unique():\n",
    "        for j in range(len(df[id_col][df[id_col]==i])):\n",
    "            start_time = df[timestamp_col][df[id_col]==i].iloc[0]\n",
    "            start_time_per_id.append(start_time)\n",
    "    df[\"start_time_per_id\"] = start_time_per_id\n",
    "    start_time_index = df.columns.get_loc(\"start_time_per_id\")\n",
    "    \n",
    "    durations_since_start = []\n",
    "    for i in range(len(df)):\n",
    "        start_time = df.iloc[i, start_time_index]\n",
    "        act_timestamp = df.iloc[i, time_index]\n",
    "        durations_since_start.append((act_timestamp - start_time).total_seconds())\n",
    "    df[\"time_since_start\"] = durations_since_start\n",
    "    dur_since_start_index = df.columns.get_loc(\"time_since_start\")\n",
    "        \n",
    "    \n",
    "    df = df.iloc[::2, [act_index, id_index, dur_index, dur_since_start_index]].reset_index(drop=True)\n",
    "    \n",
    "    return df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dataframe\n",
    "datatypes = {\"activity\" : \"category\", \"status\" : \"category\", \"id\" : \"category\"}\n",
    "df = convert_datatypes(org_df, datatypes, \"timestamp\", \"activity\", \"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the maximum trace length\n",
    "modeID = df.id.mode()[0]\n",
    "max_trace_len = df.id[df.id == modeID].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the dataframe \n",
    "print(\"Length of dataset:\", len(df), \"rows\")\n",
    "print(\"Different unique activities:\", len(df.activity.unique()))\n",
    "print(\"Different unique IDs:\", len(df.id.unique()))\n",
    "print(\"Average duration of a single activity:\", int(df.duration.mean()), \"seconds\")\n",
    "print(\"Maximum length of a trace:\", max_trace_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a new compressed dataset\n",
    "def create_compressed_dataset(df):\n",
    "    combined_list = []\n",
    "    ID_list = df.id.unique()\n",
    "\n",
    "    for ID in ID_list:\n",
    "        sub_df = df.id[df.id == ID].copy()\n",
    "        act_list = []\n",
    "        total_time = 0\n",
    "        for i in range(0, len(sub_df)):\n",
    "            act_list.append(df.activity[i])\n",
    "            total_time += int(df.duration[i])\n",
    "        id_act_time_list = [ID, act_list, total_time]\n",
    "        combined_list.append(id_act_time_list)\n",
    "    compressed_df = pd.DataFrame(combined_list, columns = [\"id\", \"activity\", \"duration\"])\n",
    "    \n",
    "    return compressed_df\n",
    "\n",
    "\n",
    "# Function to identify p% (but at least one) standard variants from a compressed dataset\n",
    "def identify_standard_variant_ids(df, p):\n",
    "    ids_standard = []\n",
    "    compressed_df = create_compressed_dataset(df)\n",
    "    compressed_df[\"activity\"] = compressed_df[\"activity\"].astype(str)\n",
    "    \n",
    "    n = int(len(compressed_df.activity.unique())* p)\n",
    "    if n == 0:\n",
    "        n = 1\n",
    "    \n",
    "    for i in range(n):\n",
    "        most_frequent = str(compressed_df[\"activity\"].mode()[0])\n",
    "        df_most_frequent = compressed_df[compressed_df[\"activity\"] == most_frequent]\n",
    "        ids_standard.extend(df_most_frequent.id.unique().tolist())\n",
    "        for j in ids_standard:\n",
    "            compressed_df.drop(compressed_df.loc[compressed_df.id == j].index, inplace = True)\n",
    "    \n",
    "    return ids_standard\n",
    "\n",
    "\n",
    "# Function to add labels to standard variants with label zero as undesirable class\n",
    "def add_class_labels(df, p):\n",
    "    ids_undesirable = identify_standard_variant_ids(df, p)\n",
    "    label_column = []\n",
    "    \n",
    "    for i in range(0, len(df)):\n",
    "        if df.id.loc[i] in ids_undesirable:\n",
    "            label_column.append(0)\n",
    "        else:\n",
    "            label_column.append(1)\n",
    "\n",
    "    labelled_df = df\n",
    "    labelled_df[\"label\"] = label_column\n",
    "  \n",
    "    return labelled_df\n",
    "\n",
    "\n",
    "# Function to build subdatasets and restore original dataset\n",
    "def build_subdatasets(df, p):\n",
    "    labelled_df = add_class_labels(df, p)\n",
    "    undesirable = labelled_df[labelled_df.label == 0]\n",
    "    undesirable = undesirable[[\"id\", \"activity\", \"duration\", \"time_since_start\"]]\n",
    "\n",
    "    positively_deviant = labelled_df[labelled_df.label == 1]\n",
    "    positively_deviant = positively_deviant[[\"id\", \"activity\", \"duration\", \"time_since_start\"]]\n",
    "    \n",
    "    df = df[[\"id\", \"activity\", \"duration\", \"time_since_start\"]]\n",
    "    \n",
    "    return df, undesirable, positively_deviant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build subdatasets\n",
    "df, undesirable, positively_deviant = build_subdatasets(df, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalize trace lengths\n",
    "def normalize_trace_length(df, max_trace_len):\n",
    "    for i in df.id.unique():\n",
    "        trace_len = df.id[df.id == i].count()\n",
    "        while trace_len < max_trace_len:\n",
    "            new_row = {\"id\": i, \"activity\": \"none\", \"duration\": 0.0, \"time_since_start\": max_time_since_start}\n",
    "            df = df.append(new_row, ignore_index=True)\n",
    "            trace_len += 1\n",
    "\n",
    "    df = df.rename_axis(\"index\").sort_values(by= [\"id\", \"index\"])\n",
    "    df = df.reset_index(drop = True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fit one hot encoder to activity column\n",
    "def fit_activity_encoder(df):\n",
    "    encoder_activity = OneHotEncoder(sparse=False)\n",
    "    encoder_activity.fit(df.activity.to_numpy().reshape(-1,1))\n",
    "    return encoder_activity\n",
    "\n",
    "\n",
    "# Function to fit MinMaxScaler on numerical values in duration column to [0,1] \n",
    "def fit_time_scalers(df):           \n",
    "    scaler_duration = MinMaxScaler()\n",
    "    scaler_duration.fit(df.duration.to_numpy().reshape(-1,1))\n",
    "    \n",
    "    scaler_time_since_start = MinMaxScaler()\n",
    "    scaler_time_since_start.fit(df.time_since_start.to_numpy().reshape(-1,1))\n",
    "    \n",
    "    return scaler_duration, scaler_time_since_start\n",
    "\n",
    "\n",
    "# Function to encode variables on original dataset with encoders fitted to the original dataset\n",
    "def encode_variables(df):\n",
    "    encoder_activity = fit_activity_encoder(df)\n",
    "    scaler_duration, scaler_time_since_start = fit_time_scalers(df)\n",
    "    id_column = df.id.to_numpy().reshape(-1,1)\n",
    "    \n",
    "    activity_transformed = encoder_activity.transform(df.activity.to_numpy().reshape(-1,1))\n",
    "    duration_transformed = scaler_duration.transform(df.duration.to_numpy().reshape(-1,1))\n",
    "    time_since_start_transformed = scaler_time_since_start.transform(df.duration.to_numpy().reshape(-1,1))\n",
    "    transformed = np.hstack((id_column, activity_transformed, duration_transformed, time_since_start_transformed))\n",
    "    \n",
    "    return transformed\n",
    "\n",
    "\n",
    "# Function to encode variables on subdatasets with encoders fitted to original dataset\n",
    "def encode_variables_sub(subdf, df):\n",
    "    encoder_activity = fit_activity_encoder(df)\n",
    "    scaler_duration, scaler_time_since_start = fit_time_scalers(df)\n",
    "    id_column = subdf.id.to_numpy().reshape(-1,1)\n",
    "    \n",
    "    activity_transformed = encoder_activity.transform(subdf.activity.to_numpy().reshape(-1,1))\n",
    "    duration_transformed = scaler_duration.transform(subdf.duration.to_numpy().reshape(-1,1))\n",
    "    time_since_start_transformed = scaler_time_since_start.transform(subdf.duration.to_numpy().reshape(-1,1))\n",
    "    transformed = np.hstack((id_column, activity_transformed, duration_transformed, time_since_start_transformed))\n",
    "    \n",
    "    return transformed\n",
    "\n",
    "\n",
    "# Function to split data into sequences according to a given trace length\n",
    "def split_data_into_traces(df, trace_len):\n",
    "    sequences = []\n",
    "    j = 0\n",
    "    for i in range(0, int((df.shape[0]+1)/trace_len)):\n",
    "        sequence = df[j : j + trace_len, 1:]                 \n",
    "        sequence = sequence.reshape(1, trace_len, -1)\n",
    "        sequences.append(sequence)\n",
    "        j += trace_len\n",
    "    sequences = np.vstack((sequences))\n",
    "    \n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing trace lengths in all datasets\n",
    "norm_df = normalize_trace_length(df, max_trace_len)\n",
    "norm_undesirable = normalize_trace_length(undesirable, max_trace_len)\n",
    "norm_positively_deviant = normalize_trace_length(positively_deviant, max_trace_len)\n",
    "\n",
    "# Encoding the data\n",
    "data_transformed = encode_variables(norm_df)\n",
    "undesirable_data_transformed = encode_variables_sub(norm_undesirable, norm_df)\n",
    "positively_deviant_data_transformed = encode_variables_sub(norm_positively_deviant, norm_df)\n",
    "\n",
    "# Splitting the data into traces   \n",
    "sequences = split_data_into_traces(data_transformed, max_trace_len)\n",
    "undesirable_sequences = split_data_into_traces(undesirable_data_transformed, max_trace_len)\n",
    "positively_deviant_sequences = split_data_into_traces(positively_deviant_data_transformed, max_trace_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Automated process improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get random noise in the correct data format\n",
    "def get_noise(n_samples, data, device):\n",
    "    z = torch.randn(n_samples, data.shape[1], data.shape[2], device = device)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator class\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, data, batch_size, hidden_size, num_layers, num_directions):\n",
    "        seq_len = data.shape[1]\n",
    "        input_size = data.shape[2]\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.h = torch.randn(num_layers * num_directions, batch_size, hidden_size)\n",
    "        self.c = torch.randn(num_layers * num_directions, batch_size, hidden_size)\n",
    "        latent_vector_size = 50 * batch_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, dropout=0.25, batch_first=True, bidirectional=False)\n",
    "\n",
    "        self.linear1 = nn.Linear(batch_size * seq_len * hidden_size, latent_vector_size)\n",
    "        self.linearHC = nn.Linear(num_layers * hidden_size * batch_size, latent_vector_size)\n",
    "        self.linearHCO = nn.Linear(3 * latent_vector_size, batch_size * seq_len * input_size)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "            \n",
    "    def forward(self, x):  \n",
    "        \n",
    "        seq_len = data.shape[1]\n",
    "        input_size = data.shape[2]\n",
    "        \n",
    "        output, (h,c) = self.lstm(x,(self.h, self.c))\n",
    "        self.h = h.detach()\n",
    "        self.c = c.detach()\n",
    "        \n",
    "        u = output.reshape((output.size()[0] * output.size()[1] * output.size()[2]))\n",
    "        u = self.relu(self.linear1(u))\n",
    "        \n",
    "        uH = F.leaky_relu(self.linearHC(h.reshape((h.size()[0] * h.size()[1] * h.size()[2]))))\n",
    "        uC = F.leaky_relu(self.linearHC(c.reshape((c.size()[0] * c.size()[1] * c.size()[2]))))\n",
    "        uHCO = torch.cat((uH, uC, u))\n",
    "        uHCO = self.linearHCO(uHCO)\n",
    "        u = uHCO\n",
    "        \n",
    "        output = u.view((output.size()[0], output.size()[1], self.input_size))\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator class\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, data, batch_size, hidden_size, num_layers, num_directions):\n",
    "        self.batch_size = batch_size \n",
    "        seq_len = data.shape[1]\n",
    "        input_size = data.shape[2]\n",
    "        \n",
    "        super().__init__()\n",
    "    \n",
    "        self.h = torch.randn(num_layers * num_directions, batch_size, hidden_size)\n",
    "        self.c = torch.randn(num_layers * num_directions, batch_size, hidden_size)\n",
    "        latent_vector_size = 50 * batch_size\n",
    "                        \n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, dropout = 0.25, batch_first = True, bidirectional = False)\n",
    "\n",
    "    \n",
    "        self.linear1 = nn.Linear(batch_size * seq_len * hidden_size, latent_vector_size)\n",
    "        self.linearHC = nn.Linear(num_layers * hidden_size * batch_size, latent_vector_size)\n",
    "        self.linearHCO = nn.Linear(3 * latent_vector_size, batch_size * seq_len * input_size)\n",
    "        self.linear2 = nn.Linear(batch_size * seq_len * input_size, 100)\n",
    "        self.linear3 = nn.Linear(100, 50)\n",
    "        self.linear4 = nn.Linear(50, batch_size)\n",
    "\n",
    "        self.linear5 = nn.Linear(batch_size * seq_len * input_size, 100)\n",
    "        self.linear6 = nn.Linear(100, 50)\n",
    "        self.linear7 = nn.Linear(50, batch_size)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        seq_len = data.shape[1]\n",
    "        input_size = data.shape[2]\n",
    "        \n",
    "        output, (h,c) = self.lstm(x, (self.h, self.c))\n",
    "        self.h = h.detach()\n",
    "        self.c = c.detach()\n",
    "        \n",
    "        u = output.reshape((output.size()[0] * output.size()[1] * output.size()[2]))\n",
    "        u = self.relu(self.linear1(u))\n",
    "        \n",
    "        uH = F.leaky_relu(self.linearHC(h.reshape((h.size()[0] * h.size()[1] * h.size()[2]))))\n",
    "        uC = F.leaky_relu(self.linearHC(c.reshape((c.size()[0] * c.size()[1] * c.size()[2]))))\n",
    "        uHCO = torch.cat((uH, uC, u))\n",
    "        uHCO = self.linearHCO(uHCO)\n",
    "        \n",
    "        # Classification to determine real vs. fake\n",
    "        u = F.relu(self.linear2(uHCO))\n",
    "        u = F.relu(self.linear3(u))\n",
    "        u = self.linear4(u)\n",
    "        output = u\n",
    "        standard_output = output.reshape((self.batch_size, -1))\n",
    "        \n",
    "        \n",
    "        # Additional classification to determine standard/deviant with a sigmoid function \n",
    "        cl = F.relu(self.linear5(uHCO))\n",
    "        cl = F.relu(self.linear6(cl))\n",
    "        cl = self.linear7(cl)\n",
    "        cl_output = cl\n",
    "        class_output = output.reshape((self.batch_size, -1))\n",
    "              \n",
    "        return standard_output, class_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train generator and discriminator\n",
    "def train(disc, gen, disc_optimizer, gen_optimizer, data, standard_data, deviant_data, epochs, batch_size, device):\n",
    " \n",
    "    current_step = 0\n",
    " \n",
    "    batches_per_epoch = int(data.shape[0] / batch_size)\n",
    "    target_nr_indices = batch_size * batches_per_epoch\n",
    "    \n",
    "    disc = disc.float()\n",
    "            \n",
    "    for epoch in range(epochs):\n",
    "        indices_list = list(range(0, data.shape[0]))\n",
    "        undesirable_list = list(range(0,undesirable_data.shape[0]))\n",
    "        positively_deviant_list = list(range(0, positively_deviant_data.shape[0]))\n",
    "                            \n",
    "        total_gen_loss_of_epoch = []\n",
    "        \n",
    "        total_disc_loss_of_epoch = [] \n",
    "        \n",
    "        if not len(undesirable_list) == target_nr_indices:\n",
    "            nr_lists = int(target_nr_indices/len(undesirable_list))\n",
    "            undesirable_indices_list = []\n",
    "            for i in range(nr_lists):\n",
    "                undesirable_indices_list.extend(undesirable_list)\n",
    "            nr_single_values = target_nr_indices - len(undesirable_indices_list)\n",
    "            values = sample(undesirable_list, nr_single_values)\n",
    "            undesirable_indices_list.extend(values)\n",
    "        else:\n",
    "            undesirable_indices_list = undesirable_list \n",
    "                            \n",
    "                            \n",
    "        if not len(positively_deviant_list) == target_nr_indices:\n",
    "            nr_lists = int(target_nr_indices/len(positively_deviant_list))\n",
    "            positively_deviant_indices_list = []\n",
    "            for i in range(nr_lists):\n",
    "                positively_deviant_indices_list.extend(positively_deviant_list)\n",
    "            nr_single_values = target_nr_indices - len(positively_deviant_indices_list)\n",
    "            values = sample(positively_deviant_list, nr_single_values)\n",
    "            positively_deviant_indices_list.extend(values)\n",
    "        else:\n",
    "            positively_deviant_indices_list = positively_deviant_list\n",
    "                                                                 \n",
    "                                    \n",
    "        for batch in range(batches_per_epoch):\n",
    "     \n",
    "            # Update Discriminator\n",
    "            disc_optimizer.zero_grad()\n",
    "            \n",
    "            ## Loss for real vs. fake classification \n",
    "            ### Produce synthetic sequences \n",
    "            z = get_noise(batch_size, data, device)\n",
    "            fake = gen(z)\n",
    "            \n",
    "            ### Get real data batch                \n",
    "            batch_indices = sample(indices_list, batch_size)\n",
    "            for i in batch_indices:\n",
    "                indices_list.remove(i)\n",
    "            real = data[batch_indices]\n",
    "            real = real.astype(\"float64\")\n",
    "            real = torch.from_numpy(real)  \n",
    "            real = real.to(device)\n",
    "            \n",
    "            ### Get prediction results and calculate classification loss\n",
    "            disc_fake_pred, non_necessary_output = disc(fake.detach())\n",
    "            disc_fake_loss = criterion(torch.sigmoid(disc_fake_pred), torch.zeros_like(disc_fake_pred))  \n",
    "           \n",
    "            disc_real_pred, non_necessary_output = disc(real.float())\n",
    "            disc_real_loss = criterion(torch.sigmoid(disc_real_pred), torch.ones_like(disc_real_pred))   \n",
    "            \n",
    "            real_fake_disc_loss = (disc_fake_loss + disc_real_loss)/2    \n",
    "\n",
    "            ### Update gradients\n",
    "            real_fake_disc_loss.backward(retain_graph = True)\n",
    "            \n",
    "            \n",
    "            ## Loss for undesirable vs. positively deviant classification\n",
    "            ### Get real undesirable batch\n",
    "            undesirable_batch_indices = sample(undesirable_indices_list, batch_size)\n",
    "            for i in undesirable_batch_indices:\n",
    "                undesirable_indices_list.remove(i)\n",
    "            undesirable = undesirable_data[undesirable_batch_indices]\n",
    "            undesirable = undesirable.astype(\"float64\")\n",
    "            undesirable = torch.from_numpy(undesirable)\n",
    "            undesirable = undesirable.to(device)\n",
    "            \n",
    "            ### Get real positively deviant batch\n",
    "            positively_deviant_batch_indices = sample(positively_deviant_indices_list, batch_size)\n",
    "            for i in positively_deviant_batch_indices:\n",
    "                positively_deviant_indices_list.remove(i)                \n",
    "            positively_deviant = positively_deviant_data[positively_deviant_batch_indices]\n",
    "            positively_deviant = positively_deviant.astype(\"float64\")\n",
    "            positively_deviant = torch.from_numpy(positively_deviant)\n",
    "            positively_deviant = positively_deviant.to(device)\n",
    "            \n",
    "            ### Get prediction results and calculate classification loss \n",
    "            non_necessary_output, disc_undesirable_pred = disc(undesirable.float())\n",
    "            disc_undesirable_loss = criterion(torch.sigmoid(disc_undesirable_pred), torch.zeros_like(disc_undesirable_pred))\n",
    "            \n",
    "            non_necessary_output, disc_positively_deviant_pred = disc(positively_deviant.float())\n",
    "            disc_positively_deviant_loss = criterion(torch.sigmoid(disc_positively_deviant_pred), torch.ones_like(disc_positively_deviant_pred))\n",
    "            \n",
    "            undesirable_positively_deviant_loss = (disc_undesirable_loss + disc_positively_deviant_loss)/2\n",
    "            \n",
    "            ### Update gradients\n",
    "            undesirable_positively_deviant_loss.backward(retain_graph = True)\n",
    "            \n",
    "            ## Update optimizer\n",
    "            disc_optimizer.step()\n",
    "            \n",
    "            ## Calculate total discriminator loss and keep track of average loss\n",
    "            disc_loss = (real_fake_disc_loss + undesirable_positively_deviant_loss)/2\n",
    "            total_disc_loss_of_epoch.append(disc_loss)\n",
    "\n",
    "            \n",
    "            \n",
    "            # Update Generator\n",
    "            gen_optimizer.zero_grad()\n",
    "                        \n",
    "            ## Get prediction results\n",
    "            disc_fake_pred, disc_class_pred = disc(fake)\n",
    "            \n",
    "            ## Loss for similarity (real vs. fake classification)\n",
    "            sim_gen_loss = criterion(torch.sigmoid(disc_fake_pred), torch.ones_like(disc_fake_pred))   \n",
    "            \n",
    "            ## Loss for innovation (undesirable vs. positively deviant) \n",
    "            undesirable_gen_loss = criterion(torch.sigmoid(disc_class_pred), torch.zeros_like(disc_class_pred))\n",
    "            if undesirable_gen_loss < 0:\n",
    "                undesirable_gen_loss = 0\n",
    "                print(f\"Epoch {current_step}: inno_loss had to be adjusted to 1\") \n",
    "            elif undesirable_gen_loss >1:\n",
    "                undesirable_gen_loss = 1\n",
    "                print(f\"Epoch {current_step}: inno_loss had to be adjusted to 0\")\n",
    "            \n",
    "            inno_gen_loss = 1-undesirable_gen_loss\n",
    "            \n",
    "            \n",
    "            ## Calculate total generator loss and keep track of average loss\n",
    "            gen_loss = (3*sim_gen_loss + inno_gen_loss)/4\n",
    "            total_gen_loss_of_epoch.append(gen_loss)\n",
    "            \n",
    "            ## Update gradients and optimizer\n",
    "            gen_loss.backward()\n",
    "            gen_optimizer.step()\n",
    "            \n",
    "            \n",
    "        # Visualization            \n",
    "        if current_step % display_step == 0 and current_step > 0:\n",
    "            mean_gen_loss = sum(total_gen_loss_of_epoch)/len(total_gen_loss_of_epoch)\n",
    "            mean_disc_loss = sum(total_disc_loss_of_epoch)/len(total_disc_loss_of_epoch)\n",
    "            print(f\"Epoch {current_step}: Generator loss: {mean_gen_loss}, Discriminator loss: {mean_disc_loss}\")       \n",
    "        current_step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters\n",
    "beta1 = 0.5\n",
    "beta2 = 0.999\n",
    "betas = ((beta1, beta2))\n",
    "lr = 0.0002 \n",
    "device = \"cpu\"\n",
    "criterion = nn.BCELoss() \n",
    "batch_size = 10                                    \n",
    "epochs = 50 \n",
    "display_step = 5\n",
    "\n",
    "# Define the input data\n",
    "data = sequences             \n",
    "standard_data = standard_sequences\n",
    "deviant_data = deviant_sequences\n",
    "seq_len = data.shape[1]\n",
    "input_size = data.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize generator, discriminator and optimizers\n",
    "gen = Generator(data = data, \n",
    "                batch_size = batch_size, \n",
    "                hidden_size = 2 * data.shape[2], \n",
    "                num_layers = 2, \n",
    "                num_directions = 1).to(device)\n",
    "\n",
    "disc = Discriminator(data = data, \n",
    "                     batch_size = batch_size, \n",
    "                     hidden_size = 2 * data.shape[2], \n",
    "                     num_layers = 2, \n",
    "                     num_directions = 1).to(device)\n",
    " \n",
    "\n",
    "    gen_optimizer = torch.optim.Adam(gen.parameters(), lr=lr, betas = betas)\n",
    "disc_optimizer = torch.optim.Adam(disc.parameters(), lr=lr, betas = betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "train(disc = disc, \n",
    "      gen = gen, \n",
    "      disc_optimizer = disc_optimizer, \n",
    "      gen_optimizer = gen_optimizer, \n",
    "      data = data, \n",
    "      standard_data = standard_data, \n",
    "      deviant_data = deviant_data, \n",
    "      epochs = epochs, \n",
    "      batch_size = batch_size, \n",
    "      device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Output and data postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a new synthetic batch\n",
    "def create_synthetic_batch(batch_size, data, gen, device):\n",
    "    z = get_noise(batch_size, data, device)\n",
    "    synthetic_data = gen(z)\n",
    "    synthetic_data = synthetic_data.detach().numpy()\n",
    "    \n",
    "    return synthetic_data\n",
    "    \n",
    "    \n",
    "# Function to decode results\n",
    "def decode_results(synthetic_data, df):\n",
    "    encoder_activity = fit_activity_encoder(df)\n",
    "    scaler_duration, scaler_time_since_start = fit_time_scalers(df)\n",
    "    decoded_data = []\n",
    "    \n",
    "    for i in range(synthetic_data.shape[0]):\n",
    "        for j in range(synthetic_data.shape[1]):\n",
    "            temp = synthetic_data[i,j]\n",
    "            \n",
    "            activity_temp = temp[0:len(df.activity.unique())]\n",
    "            duration_temp = temp[len(df.activity.unique()):-1]\n",
    "            time_since_start_temp = temp[-1]\n",
    "            \n",
    "            activity_inverse = encoder_activity.inverse_transform(activity_temp.reshape(1,-1))[0][0]\n",
    "            duration_inverse = scaler_duration.inverse_transform(duration_temp.reshape(1,-1))[0][0]\n",
    "            time_since_start_inverse = scaler_time_since_start.inverse_transform(time_since_start_temp.reshape(1,-1))[0][0]\n",
    "            \n",
    "            decoded_data.append([activity_inverse, duration_inverse, time_since_start_inverse])   \n",
    "    \n",
    "    decoded_data = pd.DataFrame(decoded_data, columns = [\"activity\", \"duration\", \"time_since_start\"])\n",
    "    \n",
    "    return decoded_data\n",
    "\n",
    "\n",
    "# Function to correct negative durations\n",
    "def correct_negative_durations(decoded_data):\n",
    "    for i in range(len(decoded_data)):\n",
    "        decoded_data.duration[i] = abs(float(df.duration[i]))\n",
    "        decoded_data.time_since_start[i] = abs(float(df.time_since_start[i]))\n",
    "    \n",
    "    return decoded_data\n",
    "    \n",
    "\n",
    "# Function to add new IDs\n",
    "def add_IDs(decoded_data, trace_len):\n",
    "    id_column = []\n",
    "    for i in range(int(len(decoded_data)/trace_len)):\n",
    "        for j in range(trace_len):\n",
    "            id_column.append(i)\n",
    "    new_data = decoded_data\n",
    "    new_data[\"ID\"] = id_column\n",
    "    \n",
    "    return new_data\n",
    "    \n",
    "\n",
    "# Function to add new timestamps (for processes without idle time, GAN-generated and sequential timestamps)\n",
    "def add_timestamps(data_with_id, trace_len, start_time):\n",
    "    start_indices = []\n",
    "    for i in range(int(len(data_with_id)/trace_len)):\n",
    "        j = 0\n",
    "        start_indices.append(j + i*trace_len)\n",
    "    \n",
    "    seq_start_timestamps = []\n",
    "    seq_end_timestamps = []\n",
    "    \n",
    "    for i in start_indices:               \n",
    "        seq_start_time = start_time\n",
    "        index = i\n",
    "        for j in range(trace_len):\n",
    "            seq_start_timestamps.append(seq_start_time)\n",
    "            seq_end_timestamp = seq_start_time + timedelta(seconds = float(data_with_id.duration[index]))\n",
    "            seq_end_timestamps.append(seq_end_timestamp)\n",
    "            seq_start_time = seq_end_timestamp             \n",
    "            index = i+j+1\n",
    "    \n",
    "    calc_start_timestamps = []\n",
    "    calc_end_timestamps = []\n",
    "    \n",
    "    for i in start_indices:               \n",
    "        index = i\n",
    "        for j in range(trace_len):\n",
    "            calc_start_timestamp = start_time + timedelta(seconds = float(data_with_id.time_since_start[index]))\n",
    "            calc_start_timestamps.append(calc_start_timestamp)\n",
    "            \n",
    "            calc_end_timestamp = calc_start_timestamp + timedelta(seconds = float(data_with_id.duration[index]))\n",
    "            calc_end_timestamps.append(calc_end_timestamp)\n",
    " \n",
    "            index = i+j+1    \n",
    "       \n",
    "    data_with_timestamps = data_with_id\n",
    "    \n",
    "    data_with_timestamps[\"calculated_start_time\"] = calc_start_timestamps\n",
    "    data_with_timestamps[\"calculated_end_time\"] = calc_end_timestamps\n",
    "    \n",
    "    data_with_timestamps[\"sequential_start_time\"] = seq_start_timestamps\n",
    "    data_with_timestamps[\"sequential_end_time\"] = seq_end_timestamps\n",
    "    \n",
    "    return data_with_timestamps\n",
    "    \n",
    "    \n",
    "# Function to generate and decode new synthetic data based on original dataset and transformed data\n",
    "def generate_new_data(df, trace_len, batch_size, data, gen, device, start_time):\n",
    "    synthetic_data = create_synthetic_batch(batch_size, data, gen, device)\n",
    "    decoded_data = decode_results(synthetic_data, df)\n",
    "    corrected_decoded_data = correct_negative_durations(decoded_data)\n",
    "    data_with_id = add_IDs(corrected_decoded_data, trace_len)\n",
    "    data_with_timestamps = add_timestamps(data_with_id, trace_len, start_time)\n",
    "   \n",
    "    return data_with_timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and decode new data\n",
    "trace_len = max_trace_len \n",
    "start_time = datetime(\"Placeholder\")\n",
    "\n",
    "results = generate_new_data(df, \n",
    "                            trace_len = trace_len, \n",
    "                            batch_size = batch_size, \n",
    "                            data=data, gen=gen, \n",
    "                            device=device, \n",
    "                            start_time = start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export results as csv file\n",
    "results.to_csv(output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
